{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data_from_csv is called:  ./data\n",
      "_load_and_shuffle_data is called: file_name: step 1  ./data consumer_complaints.csv\n",
      "_load_and_shuffle_data is called: data_path: ./data/consumer_complaints.csv.zip\n",
      "column names Index(['product', 'consumer_complaint_narrative'], dtype='object')\n",
      "length of texts 66806\n",
      "length of ccn 66806\n",
      "data is loaded\n",
      "53444 13362\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module to load data.\n",
    "Consists of functions to load data from a CSV do the following:\n",
    "    - Read the required fields (texts and labels).\n",
    "    - Do any pre-processing if required. For example, make sure all label\n",
    "        values are in range [0, num_classes-1].\n",
    "    - Split the data into training and validation sets.\n",
    "    - Shuffle the training data.\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from time import gmtime, strftime\n",
    "import argparse\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "def load_data_from_csv (data_path,\n",
    "                        validation_split=0.2,\n",
    "                        seed=123):\n",
    "    \"\"\"Loads the mortgage customer complaint dataset.\n",
    "        # Arguments\n",
    "            data_path: string, path to the data directory.\n",
    "            validation_split: float, percentage of data to use for validation.\n",
    "            seed: int, seed for randomizer.\n",
    "        # Returns\n",
    "            A tuple of training and validation data.\n",
    "            Number of training samples: \n",
    "            Number of test samples: \n",
    "            Number of categories:  \n",
    "        # References\n",
    "            https://www.kaggle.com/tmorrison/mortgage-complaints/data\n",
    "            Download and uncompress archive from:\n",
    "            https://www.kaggle.com/cfpb/us-consumer-finance-complaints/downloads/consumer_complaints.csv/1\n",
    "        \"\"\"\n",
    "    print('load_data_from_csv is called: ',data_path)\n",
    "    columns = (1, 5)  # 1 - product, 5 - consumer_complaint_narrative.\n",
    "    data = _load_and_shuffle_data(data_path,'consumer_complaints.csv', columns, seed)\n",
    "    print('data is loaded')\n",
    "    print(len(data[0][0]),len(data[1][0]))\n",
    "\n",
    "    return data\n",
    "\n",
    "def _load_and_shuffle_data(data_path,\n",
    "                           file_name,\n",
    "                           cols,\n",
    "                           seed,\n",
    "                           separator=',',\n",
    "                           header=0):\n",
    "    \"\"\"Loads and shuffles the dataset using pandas.\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        file_name: string, name of the data file.\n",
    "        cols: list, columns to load from the data file.\n",
    "        seed: int, seed for randomizer.\n",
    "        separator: string, separator to use for splitting data.\n",
    "        header: int, row to use as data header.\n",
    "    \"\"\"\n",
    "    print('_load_and_shuffle_data is called: file_name: step 1 ', data_path, file_name)\n",
    "    np.random.seed(seed)\n",
    "    #data_path = os.path.join(data_path, file_name)\n",
    "    data_path = './data/consumer_complaints.csv.zip'\n",
    "    print('_load_and_shuffle_data is called: data_path:', data_path)\n",
    "    data = pd.read_csv(data_path, compression='zip',usecols=cols, sep=separator,dtype={'consumer_complaint_narrative': object})\n",
    "    data = data.dropna(axis=0, how='any')\n",
    "    print('column names', data.columns)\n",
    "    #data = pd.read_csv(data_path, compression='zip', dtype={'consumer_complaint_narrative': object})\n",
    "    data = data.reindex(np.random.permutation(data.index))\n",
    "    texts = list(data['consumer_complaint_narrative'])\n",
    "    labels = np.array(data['product'])\n",
    "    print('length of texts', len(texts))\n",
    "    print('length of ccn',len(labels))\n",
    "    return _split_training_and_validation_sets(texts, labels, .2)\n",
    "\n",
    "\n",
    "def _split_training_and_validation_sets(texts, labels, validation_split):\n",
    "    \"\"\"Splits the texts and labels into training and validation sets.\n",
    "    # Arguments\n",
    "        texts: list, text data.\n",
    "        labels: list, label data.\n",
    "        validation_split: float, percentage of data to use for validation.\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "    \"\"\"\n",
    "    num_training_samples = int((1 - validation_split) * len(texts))\n",
    "    return ((texts[:num_training_samples], labels[:num_training_samples]),\n",
    "            (texts[num_training_samples:], labels[num_training_samples:]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='./data',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the IMDb movie reviews dataset to demonstrate training n-gram model\n",
    "    data = load_data_from_csv(FLAGS.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to explore data.\n",
    "Contains functions to help study, visualize and understand datasets.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes\n",
    "\n",
    "\n",
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    print('inside get_num_words_per_sample_:')\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "\n",
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "    print('vectorizer: ',vectorizer)\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution(labels):\n",
    "    \"\"\"Plots the class distribution.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    \"\"\"\n",
    "    num_classes = get_num_classes(labels)\n",
    "    count_map = Counter(labels)\n",
    "    counts = [count_map[i] for i in range(num_classes)]\n",
    "    idx = np.arange(num_classes)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Class distribution')\n",
    "    plt.xticks(idx, idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to vectorize data.\n",
    "Converts the given training and validation texts into numerical tensors.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "# Vectorization parameters\n",
    "\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train)\n",
    "    x_val = selector.transform(x_val)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    return x_train, x_val\n",
    "\n",
    "def vectorize_labels(labels):\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    x= dict(zip(unique_labels,range(0,len(unique_labels))))\n",
    "    print('x is ',x)\n",
    "    print('label 0',labels[0])\n",
    "    print('label 1',labels[1])\n",
    "    print(x[labels[0]])\n",
    "    print(x[labels[1]])\n",
    "    y_labels = np.array([])\n",
    "    for i in range(len(labels)):\n",
    "      y_labels=np.append(y_labels,x[labels[i]])\n",
    "\n",
    "    print(len(labels))\n",
    "    print(len(y_labels))\n",
    "\n",
    "    return  y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module to create model.\n",
    "Helper functions to create a multi-layer perceptron model and a separable CNN\n",
    "model. These functions take the model hyper-parameters as input. This will\n",
    "allow us to create model instances with slightly varying architectures.\n",
    "\"\"\"\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    print('==================== BUILDING THE MLP MODEL ================================ ')\n",
    "\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    print('============================ UNITS: ', units)\n",
    "    print('============================ DROPOUT RATE: ', dropout_rate)\n",
    "    print('============================ LAYERS: ', layers)\n",
    "    print('============================ INPUT SHAPE: ', input_shape)\n",
    "    print('============================ NUM CLASSES: ', num_classes)\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        print('==================IN LOOP ========== DROPOUT RATE: ', dropout_rate)\n",
    "\n",
    "    #model.add(Flatten())\n",
    "    print('============================ OP UNITS: ', op_units)\n",
    "    print('============================ OP ACTIVATION: ', op_activation)\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time  13_Aug_2018_18_18_39\n",
      "load_data_from_csv is called:  123\n",
      "_load_and_shuffle_data is called: file_name: step 1  123 consumer_complaints.csv\n",
      "_load_and_shuffle_data is called: data_path: ./data/consumer_complaints.csv.zip\n",
      "column names Index(['product', 'consumer_complaint_narrative'], dtype='object')\n",
      "length of texts 66806\n",
      "length of ccn 66806\n",
      "data is loaded\n",
      "53444 13362\n",
      "data loaded and back to train ===============\n",
      "==================== BEGINNING THE TRAINING ================================ \n",
      "==================== VECTORIZING THE TEXTS ================================ \n",
      "x is  {'Bank account or service': 0, 'Consumer Loan': 1, 'Credit card': 2, 'Credit reporting': 3, 'Debt collection': 4, 'Money transfers': 5, 'Mortgage': 6, 'Other financial service': 7, 'Payday loan': 8, 'Prepaid card': 9, 'Student loan': 10}\n",
      "label 0 Debt collection\n",
      "label 1 Student loan\n",
      "4\n",
      "10\n",
      "53444\n",
      "53444\n",
      "x is  {'Bank account or service': 0, 'Consumer Loan': 1, 'Credit card': 2, 'Credit reporting': 3, 'Debt collection': 4, 'Money transfers': 5, 'Mortgage': 6, 'Other financial service': 7, 'Payday loan': 8, 'Prepaid card': 9, 'Student loan': 10}\n",
      "label 0 Consumer Loan\n",
      "label 1 Student loan\n",
      "1\n",
      "10\n",
      "13362\n",
      "13362\n",
      "==================== CREATING A MODEL INSTANCE ================================ \n",
      "==================== BUILDING THE MLP MODEL ================================ \n",
      "============================ UNITS:  64\n",
      "============================ DROPOUT RATE:  0.2\n",
      "============================ LAYERS:  2\n",
      "============================ INPUT SHAPE:  (20000,)\n",
      "============================ NUM CLASSES:  11\n",
      "==================IN LOOP ========== DROPOUT RATE:  0.2\n",
      "============================ OP UNITS:  11\n",
      "============================ OP ACTIVATION:  softmax\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000000001E94B438>\n",
      "==================== COMPILING THE MODEL WITH LEARNING PARAMS ================================ \n",
      "==================== TRAINING AND VALIDATING THE MODEL ================================ \n",
      "x_train   (0, 464)\t0.0551462\n",
      "  (0, 550)\t0.0527434\n",
      "  (0, 874)\t0.115209\n",
      "  (0, 877)\t0.096417\n",
      "  (0, 1045)\t0.0315774\n",
      "  (0, 1074)\t0.0792468\n",
      "  (0, 1260)\t0.0320138\n",
      "  (0, 1362)\t0.0443973\n",
      "  (0, 1912)\t0.0288533\n",
      "  (0, 1966)\t0.046552\n",
      "  (0, 2514)\t0.0281511\n",
      "  (0, 2623)\t0.0391236\n",
      "  (0, 3006)\t0.0568738\n",
      "  (0, 3229)\t0.0457861\n",
      "  (0, 3230)\t0.0734917\n",
      "  (0, 3242)\t0.0590153\n",
      "  (0, 3287)\t0.0686564\n",
      "  (0, 4143)\t0.128272\n",
      "  (0, 4146)\t0.168135\n",
      "  (0, 4239)\t0.0675773\n",
      "  (0, 4531)\t0.0667586\n",
      "  (0, 4532)\t0.0687987\n",
      "  (0, 4983)\t0.036022\n",
      "  (0, 5436)\t0.084884\n",
      "  (0, 5476)\t0.0488399\n",
      "  :\t:\n",
      "  (0, 16154)\t0.0970452\n",
      "  (0, 16167)\t0.0446211\n",
      "  (0, 16323)\t0.0750429\n",
      "  (0, 16343)\t0.105421\n",
      "  (0, 16709)\t0.0560672\n",
      "  (0, 16766)\t0.0374583\n",
      "  (0, 16848)\t0.0557239\n",
      "  (0, 16880)\t0.0361808\n",
      "  (0, 16888)\t0.0893967\n",
      "  (0, 17044)\t0.020342\n",
      "  (0, 17068)\t0.0569007\n",
      "  (0, 17264)\t0.0326036\n",
      "  (0, 17312)\t0.0315688\n",
      "  (0, 17514)\t0.0395957\n",
      "  (0, 17679)\t0.0402036\n",
      "  (0, 18725)\t0.020251\n",
      "  (0, 18830)\t0.063409\n",
      "  (0, 19157)\t0.0317405\n",
      "  (0, 19228)\t0.03515\n",
      "  (0, 19270)\t0.0638482\n",
      "  (0, 19411)\t0.0539787\n",
      "  (0, 19422)\t0.0433197\n",
      "  (0, 19593)\t0.100338\n",
      "  (0, 19658)\t0.0869216\n",
      "  (0, 19896)\t0.0684252\n",
      "train labels 4.0\n",
      "Train on 53444 samples, validate on 13362 samples\n",
      "Epoch 1/5\n",
      " - 35s - loss: 1.1642 - acc: 0.7059 - val_loss: 0.6811 - val_acc: 0.8134\n",
      "Epoch 2/5\n",
      " - 35s - loss: 0.6011 - acc: 0.8272 - val_loss: 0.5404 - val_acc: 0.8408\n",
      "Epoch 3/5\n",
      " - 35s - loss: 0.4815 - acc: 0.8575 - val_loss: 0.4887 - val_acc: 0.8535\n",
      "Epoch 4/5\n",
      " - 39s - loss: 0.4129 - acc: 0.8769 - val_loss: 0.4677 - val_acc: 0.8572\n",
      "Epoch 5/5\n",
      " - 37s - loss: 0.3664 - acc: 0.8902 - val_loss: 0.4617 - val_acc: 0.8589\n",
      "Validation accuracy: 0.8589283041371644, loss: 0.4616964583562138\n",
      "./models/compliants-mgmt-mlp_13_Aug_2018_18_22_24.h5\n",
      "end time  13_Aug_2018_18_22_24\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module to train n-gram model.\n",
    "Vectorizes training and validation texts into n-grams and uses that for\n",
    "training a n-gram model - a simple multi-layer perceptron model. We use n-gram\n",
    "model for text classification when the ratio of number of samples to number of\n",
    "words per sample for the given dataset is very small (<~1500).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "#import time\n",
    "from time import gmtime, strftime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#import build_model_kj\n",
    "#import load_data_ratna\n",
    "#import load_data_kj\n",
    "\n",
    "#import vectorize_data_kj\n",
    "#import explore_data_kj\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=5,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    print('==================== BEGINNING THE TRAINING ================================ ')\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    #num_classes = explore_data_kj.get_num_classes(train_labels)\n",
    "    #print('printing num clsses: ',num_classes)\n",
    "    num_classes = 11\n",
    "   # unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    #if len(unexpected_labels):\n",
    "   #     raise ValueError('Unexpected label values found in the validation set:'\n",
    "    #                     ' {unexpected_labels}. Please make sure that the '\n",
    "    #                     'labels in the validation set are in the same range '\n",
    "    #                     'as training labels.'.format(\n",
    "   #                          unexpected_labels=unexpected_labels))\n",
    "    print('==================== VECTORIZING THE TEXTS ================================ ')\n",
    "\n",
    "    # Vectorize texts.\n",
    "    train_labels=vectorize_labels(train_labels)\n",
    "    val_labels = vectorize_labels(val_labels)\n",
    "\n",
    "    x_train, x_val = ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "\n",
    "    print('==================== CREATING A MODEL INSTANCE ================================ ')\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    print('==================== COMPILING THE MODEL WITH LEARNING PARAMS ================================ ')\n",
    "\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate,amsgrad=True)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    print('==================== TRAINING AND VALIDATING THE MODEL ================================ ')\n",
    "    print('x_train',x_train[0])\n",
    "    print('train labels',train_labels[0])\n",
    "\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # evaluate the model\n",
    "   # scores = model.evaluate(X, Y, verbose=0)\n",
    "   # print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "\n",
    "    # Save model.\n",
    "    model_name='./models/compliants-mgmt-mlp_' +strftime(\"%d_%b_%Y_%H_%M_%S\", gmtime())+'.h5'\n",
    "    #model_name.append(model_name,str(strftime(\"%d_%b_%Y_%H_%M_%S\", gmtime())))\n",
    "    #model_name.append(model_name,'.h5')\n",
    "    print(model_name)\n",
    "    model.save(model_name)\n",
    "\n",
    "\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='./data',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the IMDb movie reviews dataset to demonstrate training n-gram model\n",
    "    #data = load_data_ratna.load_data_from_csv(FLAGS.data_dir)\n",
    "    starttime = gmtime()\n",
    "    print('start time ',strftime(\"%d_%b_%Y_%H_%M_%S\",starttime ))\n",
    "    data = load_data_from_csv(123)\n",
    "    print('data loaded and back to train ===============')\n",
    "    train_ngram_model(data)\n",
    "    endtime = gmtime()\n",
    "    print('end time ',strftime(\"%d_%b_%Y_%H_%M_%S\",endtime ))\n",
    "   # print('total time taken in mins ',(strftime(\"%M_%S\",endtime-starttime)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling predict\n",
      "Prediction Successful\n",
      "==================== PREPARTING A PREDICTION KJ ================================ \n",
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras import models\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import build_model_kj\n",
    "\n",
    "def predict_kj(input=None):\n",
    "    #model = models.Sequential()\n",
    "    model = load_model('./models/compliants-mgmt-mlp_13_Aug_2018_17_51_16.h5')\n",
    "    print('==================== PREPARTING A PREDICTION KJ ================================ ')\n",
    "    #x = np.array([])\n",
    "    #x = np.append(x,'SOMEONE USE MY INFORMATION. THIS ISNT MY ACCOUNT.')\n",
    "    print(model.predict_classes(input))\n",
    "\n",
    "\n",
    "### Feed in test data\n",
    "test = ['SOMEONE USE MY INFORMATION. THIS ISNT MY ACCOUNT.','I am not sure why I received an OTP']\n",
    "test_df = pd.DataFrame(test)\n",
    "test_data = test_df[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Calling predict')\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "    x_train, x_test = ngram_vectorize(\n",
    "        train_texts, train_labels, test_data)\n",
    "    print('Prediction Successful')\n",
    "    predict_kj(x_test)\n",
    "\n",
    "    # try this https://www.opencodez.com/python/text-classification-using-keras.htm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
